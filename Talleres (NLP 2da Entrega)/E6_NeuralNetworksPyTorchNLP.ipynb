{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tCF2QlTV4Y20"
      },
      "source": [
        "# Excercise 5\n",
        "## NLP with Pytorch ðŸ”¥"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uZ3qC2k5rtdC",
        "outputId": "a5502e1c-1740-46af-8706-f388a52da846"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (71.0.4)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.0)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (13.9.1)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.13.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "nZ6MsHNEdt6t"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report, precision_score, recall_score, f1_score, roc_auc_score\n",
        "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gP2mEHS0fOby",
        "outputId": "fb9b8620-c12d-428e-cf1f-6996e27014e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ljZqzxHl4Y21"
      },
      "source": [
        "Use keras framework to solve the below exercises.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "yq6_11pU4Y22"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import keras\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8UMYfF4NLvew"
      },
      "source": [
        "## 5.1 Predict rating of a movie using Keras\n",
        "\n",
        "**Exercise:** Use keras framework to predict rating."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "FIe7VrCjLwQP"
      },
      "outputs": [],
      "source": [
        "dataTraining = pd.read_csv('https://github.com/sergiomora03/AdvancedTopicsAnalytics/raw/main/datasets/dataTraining.zip', encoding='UTF-8', index_col=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "SmFuLoESMvmd"
      },
      "outputs": [],
      "source": [
        "plots = dataTraining['plot']\n",
        "y = (dataTraining['rating'] >= dataTraining['rating'].mean()).astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        },
        "id": "5P3fPElaMyPI",
        "outputId": "c43422b0-92c0-4898-b902-4e48a30d41d6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3107    most is the story of a single father who takes...\n",
              "900     a serial killer decides to teach the secrets o...\n",
              "6724    in sweden ,  a female blackmailer with a disfi...\n",
              "4704    in a friday afternoon in new york ,  the presi...\n",
              "2582    in los angeles ,  the editor of a publishing h...\n",
              "                              ...                        \n",
              "8417    \" our marriage ,  their wedding .  \"  it ' s l...\n",
              "1592    the wandering barbarian ,  conan ,  alongside ...\n",
              "1723    like a tale spun by scheherazade ,  kismet fol...\n",
              "7605    mrs .  brisby ,  a widowed mouse ,  lives in a...\n",
              "215     tinker bell journey far north of never land to...\n",
              "Name: plot, Length: 7895, dtype: object"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>plot</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3107</th>\n",
              "      <td>most is the story of a single father who takes...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>900</th>\n",
              "      <td>a serial killer decides to teach the secrets o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6724</th>\n",
              "      <td>in sweden ,  a female blackmailer with a disfi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4704</th>\n",
              "      <td>in a friday afternoon in new york ,  the presi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2582</th>\n",
              "      <td>in los angeles ,  the editor of a publishing h...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8417</th>\n",
              "      <td>\" our marriage ,  their wedding .  \"  it ' s l...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1592</th>\n",
              "      <td>the wandering barbarian ,  conan ,  alongside ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1723</th>\n",
              "      <td>like a tale spun by scheherazade ,  kismet fol...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7605</th>\n",
              "      <td>mrs .  brisby ,  a widowed mouse ,  lives in a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>215</th>\n",
              "      <td>tinker bell journey far north of never land to...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>7895 rows Ã— 1 columns</p>\n",
              "</div><br><label><b>dtype:</b> object</label>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        },
        "id": "hAUPXQ2eMz47",
        "outputId": "48a8821f-276f-4d34-a63f-a12382c7294b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3107    1\n",
              "900     0\n",
              "6724    1\n",
              "4704    1\n",
              "2582    1\n",
              "       ..\n",
              "8417    0\n",
              "1592    0\n",
              "1723    0\n",
              "7605    1\n",
              "215     1\n",
              "Name: rating, Length: 7895, dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>rating</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3107</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>900</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6724</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4704</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2582</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8417</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1592</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1723</th>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7605</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>215</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>7895 rows Ã— 1 columns</p>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDHYLQghM9xc"
      },
      "source": [
        "## Data Precosessing\n",
        "\n",
        "- Remove stopwords\n",
        "- Lowercase\n",
        "- split the text in words\n",
        "- pad_sequences"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Descargar las stopwords\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Ie8w0jULzBR",
        "outputId": "ee7329ab-b6d2-441c-b6bb-f43fad1f4e14"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargar y preprocesar los datos\n",
        "\n",
        "plots = dataTraining['plot']\n",
        "y = (dataTraining['rating'] >= dataTraining['rating'].mean()).astype(int)"
      ],
      "metadata": {
        "id": "J8itolbAL4Br"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Preprocesamiento para eliminar stopwords y convertir a minÃºsculas\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()  # Convertir a minÃºsculas\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Eliminar caracteres no alfabÃ©ticos\n",
        "    tokens = text.split()  # Tokenizar el texto\n",
        "    tokens = [word for word in tokens if word not in stop_words]  # Eliminar stopwords\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# Aplicar el preprocesamiento a las tramas\n",
        "plots_clean = plots.apply(preprocess_text)\n"
      ],
      "metadata": {
        "id": "dgf7ZZdML6Mz"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Usar TfidfVectorizer con el texto preprocesado\n",
        "vectorizer = TfidfVectorizer(max_features=1350)  # Puedes ajustar max_features segÃºn tu necesidad\n",
        "X = vectorizer.fit_transform(plots_clean).toarray()\n",
        "\n",
        "# Dividir los datos en entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "b2mV2hI4L94r"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVQrmkfCNTDz"
      },
      "source": [
        "## Build Model\n",
        "\n",
        "Create a neural network to predict the rating of a movie, calculate the testing set accuracy."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir un Dataset personalizado para PyTorch\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.tensor(X, dtype=torch.float32)\n",
        "        self.y = torch.tensor(y.values, dtype=torch.float32)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "# Crear DataLoaders\n",
        "train_dataset = TextDataset(X_train, y_train)\n",
        "test_dataset = TextDataset(X_test, y_test)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)"
      ],
      "metadata": {
        "id": "8q7C-V6wMOMu"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir el modelo de red neuronal\n",
        "class TextClassificationModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(TextClassificationModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return self.sigmoid(x)"
      ],
      "metadata": {
        "id": "2wWs3CJNMZHU"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ParÃ¡metros del modelo\n",
        "input_dim = X.shape[1]  # NÃºmero de caracterÃ­sticas del vectorizador\n",
        "hidden_dim = 128\n",
        "output_dim = 1\n",
        "\n",
        "# Inicializar el modelo, la funciÃ³n de pÃ©rdida y el optimizador\n",
        "model = TextClassificationModel(input_dim, hidden_dim, output_dim)\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.00001)\n",
        "\n",
        "# Entrenamiento del modelo\n",
        "num_epochs = 100\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for texts, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(texts)\n",
        "        loss = criterion(outputs.squeeze(), labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}\")\n",
        "\n",
        "# EvaluaciÃ³n en el conjunto de prueba\n",
        "model.eval()\n",
        "all_labels = []\n",
        "all_predictions = []\n",
        "with torch.no_grad():\n",
        "    for texts, labels in test_loader:\n",
        "        outputs = model(texts)\n",
        "        predicted = (outputs.squeeze() > 0.5).float()\n",
        "        all_labels.extend(labels.numpy())\n",
        "        all_predictions.extend(outputs.squeeze().numpy())\n",
        "\n",
        "# Calcular mÃ©tricas\n",
        "accuracy = accuracy_score(all_labels, (np.array(all_predictions) > 0.5).astype(int))\n",
        "precision = precision_score(all_labels, (np.array(all_predictions) > 0.5).astype(int), average='weighted')\n",
        "recall = recall_score(all_labels, (np.array(all_predictions) > 0.5).astype(int), average='weighted')\n",
        "f1 = f1_score(all_labels, (np.array(all_predictions) > 0.5).astype(int), average='weighted')\n",
        "auc = roc_auc_score(all_labels, all_predictions)\n",
        "\n",
        "# Resultados\n",
        "results = {\n",
        "    \"accuracy\": accuracy,\n",
        "    \"precision\": precision,\n",
        "    \"recall\": recall,\n",
        "    \"f1_score\": f1,\n",
        "    \"auc\": auc\n",
        "}\n",
        "\n",
        "print(results)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h40C9GAkMaMZ",
        "outputId": "c0b28bd5-95b8-4323-d522-5398554fb672"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/100], Loss: 0.6951\n",
            "Epoch [2/100], Loss: 0.6941\n",
            "Epoch [3/100], Loss: 0.6932\n",
            "Epoch [4/100], Loss: 0.6923\n",
            "Epoch [5/100], Loss: 0.6913\n",
            "Epoch [6/100], Loss: 0.6902\n",
            "Epoch [7/100], Loss: 0.6891\n",
            "Epoch [8/100], Loss: 0.6879\n",
            "Epoch [9/100], Loss: 0.6866\n",
            "Epoch [10/100], Loss: 0.6852\n",
            "Epoch [11/100], Loss: 0.6838\n",
            "Epoch [12/100], Loss: 0.6823\n",
            "Epoch [13/100], Loss: 0.6806\n",
            "Epoch [14/100], Loss: 0.6789\n",
            "Epoch [15/100], Loss: 0.6770\n",
            "Epoch [16/100], Loss: 0.6751\n",
            "Epoch [17/100], Loss: 0.6730\n",
            "Epoch [18/100], Loss: 0.6709\n",
            "Epoch [19/100], Loss: 0.6686\n",
            "Epoch [20/100], Loss: 0.6663\n",
            "Epoch [21/100], Loss: 0.6639\n",
            "Epoch [22/100], Loss: 0.6614\n",
            "Epoch [23/100], Loss: 0.6588\n",
            "Epoch [24/100], Loss: 0.6561\n",
            "Epoch [25/100], Loss: 0.6534\n",
            "Epoch [26/100], Loss: 0.6507\n",
            "Epoch [27/100], Loss: 0.6479\n",
            "Epoch [28/100], Loss: 0.6452\n",
            "Epoch [29/100], Loss: 0.6424\n",
            "Epoch [30/100], Loss: 0.6396\n",
            "Epoch [31/100], Loss: 0.6367\n",
            "Epoch [32/100], Loss: 0.6339\n",
            "Epoch [33/100], Loss: 0.6310\n",
            "Epoch [34/100], Loss: 0.6282\n",
            "Epoch [35/100], Loss: 0.6254\n",
            "Epoch [36/100], Loss: 0.6226\n",
            "Epoch [37/100], Loss: 0.6199\n",
            "Epoch [38/100], Loss: 0.6171\n",
            "Epoch [39/100], Loss: 0.6145\n",
            "Epoch [40/100], Loss: 0.6118\n",
            "Epoch [41/100], Loss: 0.6091\n",
            "Epoch [42/100], Loss: 0.6066\n",
            "Epoch [43/100], Loss: 0.6042\n",
            "Epoch [44/100], Loss: 0.6017\n",
            "Epoch [45/100], Loss: 0.5991\n",
            "Epoch [46/100], Loss: 0.5969\n",
            "Epoch [47/100], Loss: 0.5945\n",
            "Epoch [48/100], Loss: 0.5922\n",
            "Epoch [49/100], Loss: 0.5900\n",
            "Epoch [50/100], Loss: 0.5877\n",
            "Epoch [51/100], Loss: 0.5855\n",
            "Epoch [52/100], Loss: 0.5836\n",
            "Epoch [53/100], Loss: 0.5813\n",
            "Epoch [54/100], Loss: 0.5794\n",
            "Epoch [55/100], Loss: 0.5775\n",
            "Epoch [56/100], Loss: 0.5756\n",
            "Epoch [57/100], Loss: 0.5737\n",
            "Epoch [58/100], Loss: 0.5718\n",
            "Epoch [59/100], Loss: 0.5701\n",
            "Epoch [60/100], Loss: 0.5682\n",
            "Epoch [61/100], Loss: 0.5666\n",
            "Epoch [62/100], Loss: 0.5651\n",
            "Epoch [63/100], Loss: 0.5634\n",
            "Epoch [64/100], Loss: 0.5619\n",
            "Epoch [65/100], Loss: 0.5602\n",
            "Epoch [66/100], Loss: 0.5586\n",
            "Epoch [67/100], Loss: 0.5575\n",
            "Epoch [68/100], Loss: 0.5560\n",
            "Epoch [69/100], Loss: 0.5545\n",
            "Epoch [70/100], Loss: 0.5531\n",
            "Epoch [71/100], Loss: 0.5519\n",
            "Epoch [72/100], Loss: 0.5505\n",
            "Epoch [73/100], Loss: 0.5492\n",
            "Epoch [74/100], Loss: 0.5481\n",
            "Epoch [75/100], Loss: 0.5468\n",
            "Epoch [76/100], Loss: 0.5454\n",
            "Epoch [77/100], Loss: 0.5443\n",
            "Epoch [78/100], Loss: 0.5432\n",
            "Epoch [79/100], Loss: 0.5419\n",
            "Epoch [80/100], Loss: 0.5409\n",
            "Epoch [81/100], Loss: 0.5398\n",
            "Epoch [82/100], Loss: 0.5388\n",
            "Epoch [83/100], Loss: 0.5376\n",
            "Epoch [84/100], Loss: 0.5365\n",
            "Epoch [85/100], Loss: 0.5354\n",
            "Epoch [86/100], Loss: 0.5345\n",
            "Epoch [87/100], Loss: 0.5336\n",
            "Epoch [88/100], Loss: 0.5330\n",
            "Epoch [89/100], Loss: 0.5317\n",
            "Epoch [90/100], Loss: 0.5308\n",
            "Epoch [91/100], Loss: 0.5299\n",
            "Epoch [92/100], Loss: 0.5290\n",
            "Epoch [93/100], Loss: 0.5282\n",
            "Epoch [94/100], Loss: 0.5273\n",
            "Epoch [95/100], Loss: 0.5264\n",
            "Epoch [96/100], Loss: 0.5258\n",
            "Epoch [97/100], Loss: 0.5250\n",
            "Epoch [98/100], Loss: 0.5240\n",
            "Epoch [99/100], Loss: 0.5232\n",
            "Epoch [100/100], Loss: 0.5225\n",
            "{'accuracy': 0.6333122229259025, 'precision': 0.6330084743837028, 'recall': 0.6333122229259025, 'f1_score': 0.6331479153212681, 'auc': 0.680089376791179}\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}